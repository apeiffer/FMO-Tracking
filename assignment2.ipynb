{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YQsvYOhY_ULp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "753675e19ba5435d85b16757b3f0ee9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e2e85642c0a548f9afc5ae2ae5df3511",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0cfdb7f2c6ea461c940219d9b3ffa3c9",
              "IPY_MODEL_f778d68c9c9248d6a5178e829f242150"
            ]
          }
        },
        "e2e85642c0a548f9afc5ae2ae5df3511": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0cfdb7f2c6ea461c940219d9b3ffa3c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c18345dce8ac47a1abe6fde8c9b0c382",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_29cc935167b341aaa35269cbe87cf9c1"
          }
        },
        "f778d68c9c9248d6a5178e829f242150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5fe06fe1f45a42e4bf63a7beece76376",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:20&lt;00:00, 27.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_201b54865536455da91528f1c58c494a"
          }
        },
        "c18345dce8ac47a1abe6fde8c9b0c382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "29cc935167b341aaa35269cbe87cf9c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5fe06fe1f45a42e4bf63a7beece76376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "201b54865536455da91528f1c58c494a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccf23/FMO-Tracking/blob/main/assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQsvYOhY_ULp"
      },
      "source": [
        "\r\n",
        "# Part A: Loading and Using a Pretrained Network as a Feature Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9etLuaTNofP4"
      },
      "source": [
        "1. Import required modules and libraries and mount Google Drive for access of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHRnMXnmCdD3"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.optim import lr_scheduler\r\n",
        "import numpy as np\r\n",
        "import torchvision\r\n",
        "from torchvision import datasets, models, transforms\r\n",
        "import time\r\n",
        "import os\r\n",
        "import copy\r\n",
        "from sklearn import svm\r\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.pipeline import make_pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZM_sHc4rFHH",
        "outputId": "de3c7b02-e4cd-4d2c-9dae-3bfe268e11b6"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "os.chdir('/content/drive/MyDrive/cs2770_hw2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXnXVqCGon4l"
      },
      "source": [
        "2. Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61jedQzfCp3t"
      },
      "source": [
        "# transforms.Normalize normalizes tensor values based on mean and standard deviation for RGB values\r\n",
        "# [0.485, 0.456, 0.406] are the means for RGB channels\r\n",
        "# [0.229, 0.224, 0.225] are the standard deviations for RGB channels\r\n",
        "\r\n",
        "data_transforms = {\r\n",
        "    'train': transforms.Compose([\r\n",
        "      transforms.Resize((224, 224)),\r\n",
        "      transforms.ToTensor(),\r\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                              \r\n",
        "    ]),\r\n",
        "    'val': transforms.Compose([\r\n",
        "      transforms.Resize((224, 224)),\r\n",
        "      transforms.ToTensor(),\r\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\r\n",
        "    ]),\r\n",
        "    'test': transforms.Compose([\r\n",
        "      transforms.Resize((224, 224)),\r\n",
        "      transforms.ToTensor(),\r\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\r\n",
        "    ])\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA222EA2o0FF"
      },
      "source": [
        "3. Create data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8zHroG_Hmm0",
        "outputId": "169aa089-8861-4584-ffbb-e36f2a8eee4f"
      },
      "source": [
        "data_dir = 'hw2_data'\r\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\r\n",
        "  for x in ['train', 'val', 'test']}\r\n",
        "\r\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=8, shuffle=True, num_workers=4)\r\n",
        "  for x in ['train', 'val', 'test']}\r\n",
        "  \r\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\r\n",
        "class_names = image_datasets['train'].classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20T8ZIgSo4d6"
      },
      "source": [
        "4. Load pretrained CNN model and use `VGG16_Feature_Extraction` model to extract features of images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5B0uwgOhOD0"
      },
      "source": [
        "class VGG16_Feature_Extraction(torch.nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(VGG16_Feature_Extraction, self).__init__()\r\n",
        "    VGG16_Pretrained = models.vgg16(pretrained=True)\r\n",
        "    self.features = VGG16_Pretrained.features\r\n",
        "    self.avgpool = VGG16_Pretrained.avgpool\r\n",
        "    self.feature_extractor = nn.Sequential(*[VGG16_Pretrained.classifier[i] for i in range(6)])\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = self.features(x)\r\n",
        "    x = self.avgpool(x)\r\n",
        "    x = torch.flatten(x, 1)\r\n",
        "    x = self.feature_extractor(x)\r\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG7NX3pIiOGz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "753675e19ba5435d85b16757b3f0ee9c",
            "e2e85642c0a548f9afc5ae2ae5df3511",
            "0cfdb7f2c6ea461c940219d9b3ffa3c9",
            "f778d68c9c9248d6a5178e829f242150",
            "c18345dce8ac47a1abe6fde8c9b0c382",
            "29cc935167b341aaa35269cbe87cf9c1",
            "5fe06fe1f45a42e4bf63a7beece76376",
            "201b54865536455da91528f1c58c494a"
          ]
        },
        "outputId": "8f7cc8ad-a24d-483d-8521-3f36902dd3b8"
      },
      "source": [
        "model = VGG16_Feature_Extraction()\r\n",
        "device = 'cuda:0'\r\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "753675e19ba5435d85b16757b3f0ee9c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kvzgLwypLHi"
      },
      "source": [
        "5. Use model to extract features of images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tFuWY0ZiPcX",
        "outputId": "c64cb3ff-a8d3-4480-e892-80f94d823bef"
      },
      "source": [
        "image_features = {}\r\n",
        "image_labels = {}\r\n",
        "\r\n",
        "for phase in ['train', 'test']:\r\n",
        "  for inputs, labels in dataloaders[phase]:\r\n",
        "    inputs = inputs.to(device)\r\n",
        "    model_prediction = model(inputs)\r\n",
        "    model_prediction_numpy = model_prediction.cpu().detach().numpy()\r\n",
        "    if (phase not in image_features):\r\n",
        "      image_features[phase] = model_prediction_numpy\r\n",
        "      image_labels[phase] = labels.numpy()\r\n",
        "    else:\r\n",
        "      image_features[phase] = np.concatenate((image_features[phase], model_prediction_numpy), axis=0)\r\n",
        "      image_labels[phase] = np.concatenate((image_labels[phase], labels.numpy()), axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH-Bvg96jW0a"
      },
      "source": [
        "6. Train the network on the training data after scaling it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9Bx8jETGsMj",
        "outputId": "13675892-dee9-4cbc-e1e4-0c3af8e12210"
      },
      "source": [
        "clf = make_pipeline(StandardScaler(), svm.LinearSVC(random_state=0, tol=1e-5))\r\n",
        "clf.fit(image_features['train'], image_labels['train'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('standardscaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('linearsvc',\n",
              "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='squared_hinge', max_iter=1000,\n",
              "                           multi_class='ovr', penalty='l2', random_state=0,\n",
              "                           tol=1e-05, verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnJ9tgC1pUPN"
      },
      "source": [
        "7. Test network on test set and report accuracy. Display confusion matrix to see which classes are being incorrectly predicted the most. \r\n",
        "\r\n",
        "  Some of the most often mispredicted labels are:\r\n",
        "*   Cars are being labeled as horses.\r\n",
        "*   Tables are being labeled as TV monitors, sofas, or potted plants.\r\n",
        "*   People are being labeled as buses and sofas.\r\n",
        "*   Sofas are being labeled as people.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bua2N6uplVWM",
        "outputId": "8a4cb267-461a-4197-b36b-e997e023a45d"
      },
      "source": [
        "correct = 0\r\n",
        "\r\n",
        "for p, t in zip(clf.predict(image_features['test']), image_labels['test']):\r\n",
        "  if p == t:\r\n",
        "    correct += 1\r\n",
        "\r\n",
        "print('Accuracy of SVM: ' + str(round((100 * correct / image_labels['test'].size), 2)) + '%.')\r\n",
        "print(confusion_matrix(clf.predict(image_features['test']), image_labels['test']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of SVM: 53.57%.\n",
            "[[22  0  0  1  0  2  1  1  0  0  0  0  0  0  0  0  0  0  1  1]\n",
            " [ 0 13  0  0  0  0  0  0  2  0  1  2  0  1  0  2  1  0  2  0]\n",
            " [ 0  0 20  0  0  0  0  1  0  1  0  0  0  0  0  0  2  0  0  0]\n",
            " [ 1  1  0 21  0  1  0  0  0  1  0  2  2  0  1  0  1  0  1  0]\n",
            " [ 0  0  0  1  5  0  0  1  1  0  3  1  0  0  3  2  0  0  1  1]\n",
            " [ 0  0  1  0  0 14  0  0  0  0  0  0  0  0  1  0  0  0  0  0]\n",
            " [ 0  1  0  0  0  3 14  0  1  1  1  0  0  2  3  0  1  0  1  0]\n",
            " [ 0  0  2  0  0  0  0 18  0  0  0  1  0  0  0  1  0  1  0  1]\n",
            " [ 1  0  0  0  2  0  1  0  4  2  3  1  0  0  2  1  0  8  0  3]\n",
            " [ 0  0  0  0  0  0  0  0  0  5  0  1  2  0  0  0  1  0  0  0]\n",
            " [ 1  1  0  0  5  0  0  0  5  0 11  1  0  0  1  4  0  2  0  2]\n",
            " [ 0  0  0  0  1  0  0  1  0  1  0 11  0  0  1  2  0  1  0  0]\n",
            " [ 0  0  0  0  1  0  0  0  0  4  0  0 15  0  1  1  0  0  0  0]\n",
            " [ 0  1  1  0  0  0  1  0  0  0  1  0  1 21  2  0  0  0  0  0]\n",
            " [ 0  3  1  0  6  0  5  0  2  0  2  3  2  1  8  1  0  2  1  1]\n",
            " [ 0  2  0  1  1  0  0  1  0  0  2  1  0  0  1  4  0  1  0  2]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0 11  1  0  0]\n",
            " [ 0  1  0  0  2  0  2  1  4  0  1  1  0  0  1  3  0  9  1  2]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17  0]\n",
            " [ 0  2  0  1  2  1  1  1  6  0  0  0  0  0  0  4  0  0  0 12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s66Fdpuw8otK"
      },
      "source": [
        "# Part B: Training and Testing the CNN on our Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYsaCVB-P6TQ"
      },
      "source": [
        "**Preparing the network**\r\n",
        "\r\n",
        "1. Load VGG16 with pretrained weights from ImageNet.\r\n",
        "2. Extract the number of input features for the last fully connected layer of the model.\r\n",
        "3. Replace the last fully connected layer with a new layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXZ0D785P28g"
      },
      "source": [
        "model = models.vgg16(pretrained=True)\r\n",
        "num_ftrs = model.classifier[6].in_features\r\n",
        "model.classifier[6] = nn.Linear(num_ftrs, len(class_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGZgGBirQydC"
      },
      "source": [
        "**Steps before starting training**\r\n",
        "\r\n",
        "4. Set number of epochs to 25.\r\n",
        "5. Send the model to the CUDA device.\r\n",
        "6. Specify the criterion for evaluating the trained model.\r\n",
        "7. Set the optimizer, learning rate, and momentum.\r\n",
        "8. Create a scheduler to control the way that the learning rate changes during the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlEscWBYSXtv"
      },
      "source": [
        "**Training**\r\n",
        "\r\n",
        "9. Save the initial model weight as the best model weight and set the best accuracy as zero.\r\n",
        "10. Iterate over the epochs.\r\n",
        "11. Iterate over the train and validation set.\r\n",
        "12. Use the dataloader from previous steps to get a minibatch of images and their corresponding labels.\r\n",
        "13. Initialize the gradient vector to all zeroes.\r\n",
        "14. Use the current model weight for prediction and backpropagating the prediction loss. \r\n",
        "15. Update the scheduler status.\r\n",
        "16. Compute loss and accuracy of epoch.\r\n",
        "17. Check whether the accuracy of classification is better than the best accuracy so far to save the best model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb_a0cFXb2G8",
        "outputId": "db2a39e9-9652-4246-8797-b8b8ffbe6112"
      },
      "source": [
        "model = model.to(device)\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "epochs = 25\r\n",
        "learning_rate = 0.001\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\r\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)      \r\n",
        "\r\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\r\n",
        "best_acc = 0.0\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "  all_batchs_loss = 0.0\r\n",
        "  all_batchs_corrects = 0.0\r\n",
        "\r\n",
        "  for phase in ['train', 'val']:\r\n",
        "    if phase == 'train':\r\n",
        "      model.train()\r\n",
        "    else:\r\n",
        "      model.eval()\r\n",
        "\r\n",
        "    for inputs, labels in dataloaders[phase]: # iterating over batches\r\n",
        "      inputs = inputs.to(device)\r\n",
        "      labels = labels.to(device)\r\n",
        "\r\n",
        "      optimizer.zero_grad()\r\n",
        "\r\n",
        "      with torch.set_grad_enabled(phase == 'train'):\r\n",
        "        outputs = model(inputs)\r\n",
        "        _, preds = torch.max(outputs, 1)\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        # print(loss)\r\n",
        "        if phase == 'train':\r\n",
        "          loss.backward()\r\n",
        "          optimizer.step()\r\n",
        "      \r\n",
        "        all_batchs_loss += loss.item() * inputs.size(0)\r\n",
        "        all_batchs_corrects += torch.sum(preds == labels.data)\r\n",
        "\r\n",
        "    if phase == 'train':\r\n",
        "      scheduler.step()\r\n",
        "    \r\n",
        "    epoch_loss = all_batchs_loss / dataset_sizes[phase]\r\n",
        "    epoch_acc = all_batchs_corrects.double() / dataset_sizes[phase]\r\n",
        "\r\n",
        "    if phase == 'val' and epoch_acc > best_acc:\r\n",
        "      best_acc = epoch_acc\r\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\r\n",
        "      torch.save(best_model_wts, 'best_model_weight.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XIOYQPvmEtY"
      },
      "source": [
        "**Testing**\r\n",
        "\r\n",
        "18.   Prepare the model in the same way it was prepared for training and load the best model weight saved in training.\r\n",
        "19.   Set model to `eval` and phase to `'test'`. \r\n",
        "20. Go through the test set, predict the category of images, and compute the number of correctly classified images.\r\n",
        "21. Compute accuracy over all data.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVc-qVwnl7zL",
        "outputId": "a5afcfb0-95ab-41bf-95b3-9e9a8165ac73"
      },
      "source": [
        "model = models.vgg16()\r\n",
        "num_ftrs = model.classifier[6].in_features\r\n",
        "model.classifier[6] = nn.Linear(num_ftrs, 20)\r\n",
        "model = model.to(device)\r\n",
        "model.load_state_dict(torch.load('best_model_weight.pth'))\r\n",
        "\r\n",
        "model.eval()\r\n",
        "phase = 'test'\r\n",
        "\r\n",
        "all_labels = []\r\n",
        "all_preds = []\r\n",
        "for inputs, labels in dataloaders[phase]:\r\n",
        "  inputs = inputs.to(device)\r\n",
        "  labels = labels.to(device)\r\n",
        "  outputs = model(inputs)\r\n",
        "  _, preds = torch.max(outputs, 1)\r\n",
        "  all_batchs_corrects += torch.sum(preds == labels.data)\r\n",
        "  all_preds.append(preds.cpu().data.numpy())\r\n",
        "  all_labels.append(labels.cpu().data.numpy())\r\n",
        "\r\n",
        "all_preds = np.concatenate(all_preds)\r\n",
        "all_labels = np.concatenate(all_labels)\r\n",
        "\r\n",
        "test_acc = all_batchs_corrects.double() / dataset_sizes[phase]\r\n",
        "\r\n",
        "conf = confusion_matrix(all_labels, all_preds)\r\n",
        "\r\n",
        "print('Confusion matrix: ')\r\n",
        "print(conf)\r\n",
        "print(f\"Testing accuracy from given formula: {test_acc}\")\r\n",
        "print(f\"Testing accuracy: {np.trace(conf)/np.sum(conf)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix: \n",
            "[[25  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 12  0  0  0  0  4  0  2  0  0  0  0  5  1  0  0  0  0  1]\n",
            " [ 1  1 17  1  1  0  0  0  0  1  0  1  0  0  0  1  0  0  1  0]\n",
            " [ 1  0  0 17  1  0  2  0  0  0  0  0  0  1  1  1  0  0  1  0]\n",
            " [ 0  0  0  0 13  0  0  0  2  0  4  1  0  0  3  0  0  1  0  1]\n",
            " [ 1  0  0  0  0 17  2  0  0  0  0  0  0  0  0  0  0  0  1  0]\n",
            " [ 1  0  0  0  0  0 19  0  1  0  0  0  0  2  1  0  0  0  1  0]\n",
            " [ 0  0  0  0  1  0  0 22  0  0  0  1  0  0  0  1  0  0  0  0]\n",
            " [ 0  0  0  0  3  0  1  1 10  0  5  1  0  1  1  0  0  1  0  1]\n",
            " [ 0  0  0  0  0  0  0  0  1 11  0  0  1  1  1  0  0  0  0  0]\n",
            " [ 0  0  0  0 11  0  0  0  2  0 10  0  0  0  1  1  0  0  0  0]\n",
            " [ 0  0  1  0  1  0  1  0  1  0  0 18  0  1  1  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  2  0  0 20  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  2  0  0  0  0  0  0 22  0  0  0  0  1  0]\n",
            " [ 2  0  0  0  6  0  2  0  1  0  0  1  1  0 11  0  0  1  0  0]\n",
            " [ 0  1  0  0  1  0  1  0  4  0  3  1  0  2  2  9  0  1  0  0]\n",
            " [ 0  0  1  1  0  0  1  0  0  3  0  0  0  0  0  0 10  0  1  0]\n",
            " [ 0  0  0  0  4  0  0  1  4  0  1  2  0  0  3  2  0  8  0  0]\n",
            " [ 0  0  0  0  0  0  2  0  0  0  0  0  0  0  1  0  0  0 22  0]\n",
            " [ 0  0  0  0  3  0  0  0  4  0  0  0  0  0  1  1  0  1  0 15]]\n",
            "Testing accuracy from given formula: 7.962184873949579\n",
            "Testing accuracy: 0.6470588235294118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zim56s60b-6I"
      },
      "source": [
        "22. Repeating with Different Hyperparameters\r\n",
        "\r\n",
        "Accuracy with following variations of hyperparameters **(Base case is 25 epochs, learning rate of 0.001, and SGD optimizer)**:\r\n",
        "*   Epochs = 20: 0.6659\r\n",
        "*   Epochs = 25: 0.6197\r\n",
        "*   Epochs = 30: 0.6554\r\n",
        "\r\n",
        "\r\n",
        "*   Learning Rate = 0.001: 0.6197 \r\n",
        "*   Learning Rate = 0.0005: 0.6870\r\n",
        "\r\n",
        "\r\n",
        "*   Optimizer = SGD: 0.6197\r\n",
        "*   Optimizer = Adam: 0.0525"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a27OOEBo-d6z"
      },
      "source": [
        "# Part C: Object Detection Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKSiOx3lzPCa",
        "outputId": "0c1b96dc-320d-4109-c360-521c0723d6c3"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "os.chdir('/content/drive/MyDrive/cs2770_hw2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhiWgMhduorZ"
      },
      "source": [
        "import torch\r\n",
        "import torchvision\r\n",
        "import Required_Files.utils as utils\r\n",
        "\r\n",
        "from Required_Files.coco_utils import get_coco_api_from_dataset\r\n",
        "from Required_Files.coco_eval import CocoEvaluator\r\n",
        "import copy\r\n",
        "import torch.optim as optim\r\n",
        "from torch.optim import lr_scheduler\r\n",
        "from Required_Files.PennFudanDataset import PennFudanDataset\r\n",
        "from Required_Files.pascal_dataset import PASCALDataset\r\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkhrYS6Xr-CS",
        "outputId": "25b62ddb-e7ec-43c2-965e-e110bcfcaf3a"
      },
      "source": [
        "pascal_train = PASCALDataset('PASCAL/train')\r\n",
        "pascal_test = PASCALDataset('PASCAL/test')\r\n",
        "pascal_val = PASCALDataset('PASCAL/val')\r\n",
        "\r\n",
        "penn_fudan_train = PennFudanDataset('PennFudanPed_hw3/train')\r\n",
        "penn_fudan_test = PennFudanDataset('PennFudanPed_hw3/test')\r\n",
        "penn_fudan_val = PennFudanDataset('PennFudanPed_hw3/val')\r\n",
        "\r\n",
        "data_loader_pascal_train = torch.utils.data.DataLoader(pascal_train, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)\r\n",
        "data_loader_pascal_test = torch.utils.data.DataLoader(pascal_test, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)\r\n",
        "data_loader_pascal_val = torch.utils.data.DataLoader(pascal_val, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)\r\n",
        "\r\n",
        "data_loader_penn_fudan_train = torch.utils.data.DataLoader(penn_fudan_train, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)\r\n",
        "data_loader_penn_fudan_test = torch.utils.data.DataLoader(penn_fudan_test, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)\r\n",
        "data_loader_penn_fudan_val = torch.utils.data.DataLoader(penn_fudan_val, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT6xxSfvuYeP"
      },
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\r\n",
        "device = 'cuda:0'\r\n",
        "model = model.to(device) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pGwAbbqN17s",
        "outputId": "a02af974-de59-435e-9261-f713b3413290"
      },
      "source": [
        "num_epochs = 5\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\r\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\r\n",
        "\r\n",
        "best_mAP = 0.0\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "  for phase in ['train', 'val']:\r\n",
        "    if phase == 'train':\r\n",
        "      model.train() # just putting it in training mode and eval mode, not actually doing anything\r\n",
        "    if phase == 'val':\r\n",
        "      model.eval()\r\n",
        "\r\n",
        "    if phase == 'train':\r\n",
        "      for images, targets in data_loader_pascal_train: # batch - small number of images which are run in parallel on GPU\r\n",
        "\r\n",
        "        optimizer.zero_grad() \r\n",
        "\r\n",
        "        images = list(image.to(device) for image in images)\r\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\r\n",
        "        loss_dict = model(images, targets)\r\n",
        "\r\n",
        "        with torch.set_grad_enabled(phase == 'train'):\r\n",
        "            loss = sum(loss_dict.values())\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "    \r\n",
        "      scheduler.step()\r\n",
        "    \r\n",
        "    if phase == 'val':\r\n",
        "      coco = get_coco_api_from_dataset(data_loader_pascal_val.dataset)\r\n",
        "      iou_types = [\"bbox\"]\r\n",
        "      coco_evaluator = CocoEvaluator(coco, iou_types)\r\n",
        "    \r\n",
        "      for images, targets in data_loader_pascal_val:\r\n",
        "        images = list(image.to(device) for image in images)\r\n",
        "        outputs = model(images) # send in images and get labels\r\n",
        "\r\n",
        "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\r\n",
        "        coco_evaluator.update(res) # basically adding to the running total in the same way as was done in part b\r\n",
        "\r\n",
        "      coco_evaluator.synchronize_between_processes()\r\n",
        "      coco_evaluator.accumulate()\r\n",
        "      coco_evaluator.summarize()\r\n",
        "      mAP = coco_evaluator.coco_eval['bbox'].stats[0] # accuracy over epoch \r\n",
        "\r\n",
        "      if mAP > best_mAP:\r\n",
        "        best_mAP = mAP\r\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\r\n",
        "        torch.save(best_model_wts, 'best_model_weight.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "creating index...\n",
            "index created!\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.08s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.350\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.535\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.389\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.187\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.377\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.314\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.510\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.518\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.286\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.550\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.545\n",
            "creating index...\n",
            "index created!\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.360\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.544\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.397\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.186\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.366\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.421\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.322\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.510\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.519\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.273\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.544\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.556\n",
            "creating index...\n",
            "index created!\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.357\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.538\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.397\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.180\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.374\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.414\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.322\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.513\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.518\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.266\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.540\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.556\n",
            "creating index...\n",
            "index created!\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.353\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.535\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.403\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.176\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.367\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.409\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.318\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.500\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.509\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.260\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.532\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.549\n",
            "creating index...\n",
            "index created!\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.353\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.537\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.394\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.180\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.355\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.412\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.318\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.502\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.507\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.259\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.519\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6h2pVAp6XIl"
      },
      "source": [
        "# function for calculating mAP scores and drawing bounding boxes\r\n",
        "def update_mAP(res):\r\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EenBqnQeu6FD",
        "outputId": "5f79028e-0df8-4b38-bed9-b2400ba70977"
      },
      "source": [
        "# testing\r\n",
        "# have something here to keep a running total of the top 20 mAP scores\r\n",
        "\r\n",
        "# load the model with the best weights\r\n",
        "model = model.to(device)\r\n",
        "model.load_state_dict(torch.load('best_model_weight.pth'))\r\n",
        "\r\n",
        "coco = get_coco_api_from_dataset(data_loader_pascal_test.dataset)\r\n",
        "iou_types = [\"bbox\"]\r\n",
        "coco_evaluator = CocoEvaluator(coco, iou_types)\r\n",
        "\r\n",
        "for images, targets in data_loader_pascal_test:\r\n",
        "  images = list(image.to(device) for image in images)\r\n",
        "  outputs = model(images)\r\n",
        "\r\n",
        "  res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\r\n",
        "\r\n",
        "  # keep running total of 20 images with the largest mAP\r\n",
        "  update_mAP(res)\r\n",
        "    # add them to a dict of mAP scores\r\n",
        "    # find the 20 images with the largest mAP as well as their bounding boxes\r\n",
        "  coco_evaluator.update(res)\r\n",
        "\r\n",
        "coco_evaluator.synchronize_between_processes()\r\n",
        "coco_evaluator.accumulate()\r\n",
        "coco_evaluator.summarize()\r\n",
        "mAP = coco_evaluator.coco_eval['bbox'].stats[0]\r\n",
        "print('Overall mAP: ' + str(mAP))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.593\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.881\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.659\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.537\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.684\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.663\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.667\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.299\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.606\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.746\n",
            "Overall mAP: 0.592761734678547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUtVLQ1k6ihr"
      },
      "source": [
        "# call function to draw the bounding boxes here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}